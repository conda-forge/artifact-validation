#!/usr/bin/env python
import glob
import os
import pprint
import re
import math
from collections import defaultdict
import time
import subprocess
import copy
import threading
import difflib

import requests
import rapidjson as json
import click
import yaml
import tqdm
import joblib
from yaml.representer import Representer

from conda_forge_artifact_validation.validate import (
    download_and_validate,
)
from conda_forge_artifact_validation.glob_to_re import glob_to_re
from conda_forge_artifact_validation.utils import (
    chunk_iterable,
    split_pkg,
)
from conda_forge_artifact_validation.cached_repodata import (
    CHANNEL_URL,
    SUBDIRS,
    REPODATA_CACHE,
)

CHUNKSIZE = 64
BIG_PACKAGES = [
    "cudatoolkit",
]
CPHLock = threading.RLock()

yaml.add_representer(defaultdict, Representer.represent_dict)


def _strip_md5_or_error(data):
    any_nonmd5 = defaultdict(dict)
    for pkg_nm, art_data in data.items():
        for art, v in art_data.items():
            if len(v["bad_paths"]) == 0:
                continue
            elif all(k == "md5sum" for k in v["bad_paths"]):
                continue
            else:
                any_nonmd5[pkg_nm][art] = v
    return any_nonmd5


def _diff_res(old_data, new_data):
    old_lines = yaml.dump(
        old_data,
        default_flow_style=False,
        indent=2,
    ).splitlines()
    new_lines = yaml.dump(
        new_data,
        default_flow_style=False,
        indent=2,
    ).splitlines()
    diff_lines = []
    for ln in difflib.unified_diff(old_lines, new_lines, n=0, lineterm=''):
        if ln.startswith('+++') or ln.startswith('---') or ln.startswith('@@'):
            continue
        diff_lines.append(ln)
    return "\n".join(diff_lines)


def _test_file_paths(pkg_name, fnames, validate_yamls):
    valid = True
    bad_pths = defaultdict(list)

    for key in validate_yamls:
        if pkg_name in validate_yamls[key]["allowed"]:
            continue

        for rep, patt in zip(
            validate_yamls[key]["glob_regexes"],
            validate_yamls[key]["files"],
        ):
            for fname in fnames:
                if rep.fullmatch(fname) is not None:
                    valid = False
                    bad_pths[key].append(patt)
                    break

    return valid, bad_pths


def _munge_validate_yamls():
    # glob the validation yamls
    validate_yaml_paths = (
        glob.glob("validate_yamls/*.yaml")
        + glob.glob("generated_validate_yamls/*.generated.yaml")
    )
    validate_yamls = {}
    for pth in validate_yaml_paths:
        key = os.path.basename(pth).rsplit(".yaml", maxsplit=1)[0]
        with open(pth, "r") as fp:
            validate_yamls[key] = yaml.safe_load(fp)
    for key in validate_yamls:
        validate_yamls[key]["glob_regexes"] = [
            re.compile(glob_to_re(patt)) for patt in validate_yamls[key]["files"]
        ]
    print("found %s validate yaml files" % len(validate_yamls), flush=True)
    return validate_yamls


def _process_artifact(pkg, repodata, libcfgraph_path, subdir, validate_yamls, verbose):
    if pkg.endswith(".tar.bz2"):
        pkg_json = pkg[:-len(".tar.bz2")] + ".json"
    else:
        pkg_json = pkg

    artif_pth = os.path.join(
        "artifacts",
        repodata["name"],
        "conda-forge",
        subdir,
        pkg_json,
    )

    lcfg_pth = os.path.join(libcfgraph_path, artif_pth)
    try:
        with open(lcfg_pth, "r") as fp:
            data = json.load(fp).get("files", None)
    except Exception:
        data = None

    if data is None:
        http_url = os.path.join(
            "https://raw.githubusercontent.com/regro/libcfgraph/master",
            artif_pth,
        )
        try:
            _rr = requests.get(http_url, timeout=1)
            _rr.raise_for_status()
            data = _rr.json().get("files", None)
        except Exception:
            data = None

    if data is None:
        pkg_url = f"{CHANNEL_URL}/{subdir}/{pkg}"
        if verbose > 0:
            print("downloading artifact '%s'" % pkg_url, flush=True)

        try:
            valid, bad_pths = download_and_validate(
                CHANNEL_URL,
                f"{subdir}/{pkg}",
                validate_yamls,
                md5sum=repodata["md5"],
                lock=CPHLock,
            )
        except Exception:
            valid = False
            bad_pths = None
    else:
        valid, bad_pths = _test_file_paths(repodata["name"], data, validate_yamls)

    if not valid:
        print(
            "invalid artifact %s/%s: %s" % (
                subdir,
                pkg,
                pprint.pformat(bad_pths),
            ),
            flush=True,
        )

    if any(ss in split_pkg(os.path.join(subdir, pkg))[-1] for ss in ["py34", "py35"]):
        valid = True
        print(
            "skipping invalid artifact %s/%s due to py34/py35 in build string: %s" % (
                subdir,
                pkg,
                pprint.pformat(bad_pths)
            )
        )

    return valid, {
        repodata["name"]: {
            f"{subdir}/{pkg}": {"bad_paths": bad_pths},
        },
    }


@click.command()
@click.option(
    '--libcfgraph-path', type=str, default=None,
    help='the path to libcfgraph')
@click.option(
    '-v', '--verbose', count=True,
    help='if given, print increasing levels of output')
@click.option(
    '--time-limit', type=int, default=3600,
    help='the time limit for running in seconds')
@click.option(
    '--restart-data', type=str, default=None,
    help='a JSON blob with restart location information')
@click.option(
    '--output-path', type=str, default=None,
    help='if given, output information on invalid artifacts is append to this path')
@click.option(
    '--pull', is_flag=True,
    help='if given, pull the repo again before writing data')
def main(libcfgraph_path, verbose, time_limit, restart_data, output_path, pull):
    """Scan all conda-forge artifacts for invalid paths."""

    # do a git pull here in case repo is out of date
    if pull:
        print("pulling latest changes...", flush=True)
        subprocess.run("git pull", shell=True)

    validate_yamls = _munge_validate_yamls()
    final_data = defaultdict(dict)
    start_time = time.time()
    out_of_time = False

    curr_resdat = {"subdir": None, "pkg": None}
    if restart_data is not None:
        if os.path.exists(restart_data):
            with open(restart_data, "r") as fp:
                resdat = json.load(fp)
        else:
            resdat = {"subdir": None, "pkg": None}
    print("restart data: %s" % resdat, flush=True)
    skipped_for_restart = False

    joblib_verbose = {0: 0, 1: 0, 2: 100}[verbose]

    for subdir in SUBDIRS:
        if (
            restart_data
            and resdat["subdir"] is not None
            and SUBDIRS.index(subdir) < SUBDIRS.index(resdat["subdir"])
        ):
            continue

        print("\n" + "=" * 80, flush=True)
        print("=" * 80, flush=True)
        print("processing subdir %s" % subdir, flush=True)
        rd = REPODATA_CACHE[subdir]
        curr_resdat["subdir"] = subdir

        pkgs = sorted(rd["packages"])
        tot = math.ceil(len(pkgs) / CHUNKSIZE)
        for pkg_chunk in tqdm.tqdm(
            chunk_iterable(pkgs, CHUNKSIZE),
            total=tot,
            desc=subdir,
        ):
            if len(pkg_chunk) == 0:
                continue

            if (
                restart_data
                and resdat["pkg"] is not None
                and not skipped_for_restart
            ):
                if not any(pkg.startswith(resdat["pkg"]) for pkg in pkg_chunk):
                    continue
                else:
                    print("found restart pkg: " + resdat["pkg"], flush=True)
                    skipped_for_restart = True

            curr_resdat["pkg"] = pkg_chunk[0][0:3]
            print("curr restart pkg: " + curr_resdat["pkg"], flush=True)

            jobs = [
                joblib.delayed(_process_artifact)(
                    pkg,
                    copy.deepcopy(rd["packages"][pkg]),
                    libcfgraph_path,
                    subdir,
                    validate_yamls,
                    verbose,
                )
                for pkg in pkg_chunk
            ]
            if any(rd["packages"][pkg]["name"] in BIG_PACKAGES for pkg in pkg_chunk):
                n_jobs = 1
            else:
                n_jobs = 8

            with joblib.Parallel(
                n_jobs=n_jobs, backend='threading', verbose=joblib_verbose
            ) as para:
                any_new = False
                for d_valid, d in para(jobs):
                    if d_valid is not None and not d_valid:
                        for k, v in d.items():
                            final_data[k].update(v)
                            any_new = True

            if any_new:
                print(
                    "data:\n%s" % yaml.dump(
                        final_data,
                        default_flow_style=False,
                        indent=2,
                    ),
                    flush=True,
                )

            if time_limit is not None and time.time() - start_time >= time_limit:
                out_of_time = True
                break

        if out_of_time:
            print("\n\nout of time - stopping!\n", flush=True)
            break

    # do a git pull here in case repo is out of date
    if pull:
        print("pulling latest changes...", flush=True)
        subprocess.run("git pull", shell=True)

    if output_path is not None:
        print("writing invalid packages to '%s'..." % output_path, flush=True)
        if os.path.exists(output_path):
            with open(output_path, "r") as fp:
                old_data = yaml.safe_load(fp)
        else:
            old_data = {}

        orig_data = copy.deepcopy(old_data)

        for k, v in final_data.items():
            if k not in old_data:
                old_data[k] = {}
            old_data[k].update(final_data[k])

        # clean out things not in the main channel
        for pkg_nm in list(old_data):
            for subdir_pkg in list(old_data[pkg_nm]):
                subdir, pkg = os.path.split(subdir_pkg)
                if pkg not in REPODATA_CACHE[subdir]["packages"]:
                    del old_data[pkg_nm][subdir_pkg]

            if not old_data[pkg_nm]:
                del old_data[pkg_nm]

        with open(output_path, "w") as fp:
            fp.write(yaml.dump(old_data, default_flow_style=False, indent=2))

        diff_lines = _diff_res(
            _strip_md5_or_error(orig_data),
            _strip_md5_or_error(old_data),
        )
    else:
        diff_lines = _diff_res(
            {},
            _strip_md5_or_error(final_data),
        )

    if restart_data is not None:
        print("writing restart info to '%s'..." % restart_data, flush=True)
        with open(restart_data, "w") as fp:
            if out_of_time:
                json.dump(curr_resdat, fp)
            else:
                json.dump({"subdir": None, "pkg": None}, fp)

    if diff_lines:
        with open("scan_results.txt", "w") as fp:
            fp.write(diff_lines)


if __name__ == "__main__":
    main()
